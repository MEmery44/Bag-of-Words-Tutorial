{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, \n",
      "        25000 labeled test reviews, \n",
      "        and 50000 unlabeled reviews\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files\n",
    "train = pd.read_csv('labeledTrainData.tsv', header=0, delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv('testData.tsv', header=0, delimiter='\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv('unlabeledTrainData.tsv', header=0, delimiter='\\t', quoting=3)\n",
    "\n",
    "# Verify the numer of reviews that were read (100, 00 in total)\n",
    "print(\"\"\"Read {} labeled train reviews, \n",
    "{} labeled test reviews, \n",
    "and {} unlabeled reviews\"\"\".format(\n",
    "    train['review'].size, test['review'].size, unlabeled_train['review'].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    \"\"\"Function to convert a  document to a sequence of words, optionally removing stop words. Returns a list of words.\"\"\"\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. Remove Non-letters\n",
    "    review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [word for word in words if not word in stops]\n",
    "    return words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# Downlaod the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download()\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function ot split a review into parsed sentences\n",
    "def review_to_sentences(review, tokenzier, remove_stopwords=False):\n",
    "    \"\"\"Function to split a review into parsed sentences. Returns\n",
    "    a lsit of sentences, where each sentence is a list of words\"\"\"\n",
    "    # 1. User the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        #If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/memery/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/home/memery/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:198: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/memery/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.happierabroad.com\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/memery/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.archive.org/details/LovefromaStranger\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/memery/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.loosechangeguide.com/LooseChangeGuide.html\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/memery/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.msnbc.msn.com/id/4972055/site/newsweek/\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/memery/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:198: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/memery/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.youtube.com/watch?v=a0KSqelmgN8\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/memery/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://jake-weird.blogspot.com/2007/08/beneath.html\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "sentences = [] # Initialize an empty list of sentences\n",
    "\n",
    "print('Parsing sentences from training set')\n",
    "for review in train['review']:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "    \n",
    "print('Parsing sentences from unlabled set')\n",
    "for review in unlabeled_train['review']:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n",
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[0])\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec\n",
    "# creates nice ouput messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300 # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4 # Number of threads to run in parallel\n",
    "context = 10 # Context window size\n",
    "downsampling = 1e-3 # Downsample setting for frequence words\n",
    "\n",
    "# Initialize and train model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print('Training model...')\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers=num_workers,\n",
    "                         size=num_features,\n",
    "                         min_count=min_word_count,\n",
    "                         window=context,\n",
    "                         sample=downsampling)\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "# It can be helpful to create a meaningful model name and\n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = '300features_40minwords_10context'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('man woman child kitchen'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berlin\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('france england germany berlin'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('paris berlin london austria'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.620460033416748), ('lady', 0.5978381633758545), ('lad', 0.5947127342224121), ('millionaire', 0.5336859226226807), ('guy', 0.5209099054336548), ('monk', 0.5120577812194824), ('men', 0.5044459104537964), ('soldier', 0.5028200149536133), ('boy', 0.49186182022094727), ('person', 0.49071723222732544)]\n",
      "[('princess', 0.6938101649284363), ('bride', 0.6378064155578613), ('maid', 0.5931621789932251), ('belle', 0.5840067267417908), ('stepmother', 0.5785546898841858), ('mistress', 0.5769913792610168), ('nun', 0.5756708383560181), ('mary', 0.5738168954849243), ('goddess', 0.5737065076828003), ('victoria', 0.5731890201568604)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar('man'))\n",
    "print(model.most_similar('queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('terrible', 0.7707134485244751), ('horrible', 0.7411921620368958), ('atrocious', 0.7342128753662109), ('abysmal', 0.7259578704833984), ('dreadful', 0.703248918056488), ('appalling', 0.6884777545928955), ('horrendous', 0.6792418956756592), ('horrid', 0.6619541049003601), ('lousy', 0.6417896747589111), ('amateurish', 0.6281046867370605)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar('awful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
